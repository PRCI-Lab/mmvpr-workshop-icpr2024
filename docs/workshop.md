- **Date**: December 1st 2024
- **Time**: 17:00—20:30 PM in Beijing Time (CST)

### Agenda

<table>
  <tr>
    <th>Time (CST)</th>
    <th>Session Title</th>
  </tr>
  <tr>
    <td>17:00-17:30</td>
    <td>Workshop Reports & Challenge Results</td>
  </tr>
  <tr>
    <td>17:30-18:00</td>
    <td>Session 1: Invited Talk 1 - <strong>Dr. Han Xu</strong>, Southeast University, China</td>
  </tr>
  <tr>
    <td>18:00-18:30</td>
    <td>Session 2: Invited Talk 2 - <strong>Dr. Hui Li</strong>, Jiangnan University, China</td>
  </tr>
  <tr>
    <td>18:30-19:00</td>
    <td>Coffee Break</td>
  </tr>
  <tr>
    <td>19:00-20:30</td>
    <td>
      Session 3: Winner Presentations & Contributed Papers
      <ol>
        <li>
          <strong>Paper Title:</strong> Multi-Modal Fusion of LiDAR and PRISMA Data for Cobalt Mapping: A Case Study from the Áramo Mine, Spain⋆ <br>
          <span style="font-size: 0.9em;"><strong>Affiliations:</strong> Geological Survey of Finland (GTK), Finland; Department of Computing, University of Turku, Finland; Aurum Exploration Limited (Aurum), Kells, Ireland</span>
        </li>
        <li>
          <strong>Paper Title:</strong> Adapting SAM2 for Visual Object Tracking <br>
          <span style="font-size: 0.9em;"><strong>Affiliations:</strong> University of Washington, Seattle WA, USA; Electronics and Telecommunications Research Institute, Daejeon, South Korea; National Center for High-performance Computing, Hsinchu, Taiwan</span>
        </li>
        <li>
          <strong>Paper Title:</strong> Visual Prompt with Larger Model for Multi-Modal Tracking <br>
          <span style="font-size: 0.9em;"><strong>Affiliations:</strong> Dalian University of Technology, China; Dalian Minzu University, China</span>
        </li>
        <li>
          <strong>Paper Title:</strong> Enhancing Multi-Modal Object Detection with Data Augmentation, Focal Loss, and Model Ensembling <br>
          <span style="font-size: 0.9em;"><strong>Affiliation:</strong> State Key Laboratory for Novel Software Technology, Nanjing University, China</span>
        </li>
        <li>
          <strong>Paper Title:</strong> Advancing Multi-Modal Visual Pattern Recognition: Object Detection <br>
          <span style="font-size: 0.9em;"><strong>Affiliations:</strong> Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, Kerala; Founding Minds Software</span>
        </li>
        <li>
          <strong>Paper Title:</strong> Action Recognition Using Temporal Shift Module and Ensemble Learning <br>
          <span style="font-size: 0.9em;"><strong>Affiliations:</strong>University of Limoges, Limoges, France ; L3i Laboratory, La Rochelle University, France</span>
        </li>
        <li>
          <strong>Paper Title:</strong> Modality Fusion Adaptor-Enhanced Vision Transformer for Multimodal Action Recognition <br>
          <span style="font-size: 0.9em;"><strong>Affiliation:</strong>School of Computer Science and Technology, Xidian University, China;</span>
        </li>
        <li>
          <strong>Paper Title:</strong> An Effective End-to-End Solution for Multimodal Action Recognition <br>
          <span style="font-size: 0.9em;"><strong>Affiliations:</strong> Nanjing University, China; Nanjing University of Science and Technology, China</span>
        </li>
        <li>
          <strong>Paper Title:</strong> Evolution of Hybrid Multi-Modal Action Recognition: From DA-CNN+Bi-GRU to EfficientNet-CNN-ViT <br>
          <span style="font-size: 0.9em;"><strong>Affiliation:</strong> Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, Kerala, India;  Founding Minds Software</span>
        </li>
      </ol>
    </td>
  </tr>
  <tr>
    <td>20:30</td>
    <td>Closing Remarks</td>
  </tr>
</table>




### Meeting Info
- **Virtual link**: [Link](https://teams.microsoft.com/meet/444031699394?p=QDjHv36bIbpdvkfPeL).
- **Virtual meeting info**: <br>  _Microsoft Teams ID_: **444 031 699 394**   _Password_: **xe6iP2n7**
